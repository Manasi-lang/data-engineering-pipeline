# ğŸš€ End-to-End Data Engineering Pipeline

A comprehensive, production-ready data engineering pipeline that demonstrates best practices for extracting, transforming, and loading data using modern tools and technologies.

## ğŸ“‹ Project Overview

This project implements a complete data engineering pipeline with the following capabilities:

- **Data Extraction**: CSV files and REST APIs
- **Data Transformation**: Advanced data cleaning and feature engineering with Pandas
- **Data Loading**: PostgreSQL database with optimized schemas
- **Orchestration**: Apache Airflow for workflow management
- **Cloud Integration**: AWS S3 for storage and EC2 for deployment
- **Monitoring**: Comprehensive logging and data quality checks

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚   Transform     â”‚    â”‚   Data Store    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ CSV Files     â”‚â”€â”€â”€â–¶â”‚ â€¢ Pandas        â”‚â”€â”€â”€â–¶â”‚ â€¢ PostgreSQL    â”‚
â”‚ â€¢ REST APIs     â”‚    â”‚ â€¢ Data Cleaning â”‚    â”‚ â€¢ Analytics     â”‚
â”‚ â€¢ Streaming     â”‚    â”‚ â€¢ Validation    â”‚    â”‚ â€¢ Raw Data      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Orchestration â”‚    â”‚   Cloud Storage â”‚    â”‚   Monitoring    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Airflow DAGs  â”‚    â”‚ â€¢ AWS S3        â”‚    â”‚ â€¢ Logs          â”‚
â”‚ â€¢ Scheduling    â”‚    â”‚ â€¢ Backups       â”‚    â”‚ â€¢ Quality Checksâ”‚
â”‚ â€¢ Dependencies  â”‚    â”‚ â€¢ Archiving     â”‚    â”‚ â€¢ Alerts        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

### Core Technologies
- **Python 3.8+**: Primary programming language
- **Pandas**: Data manipulation and analysis
- **SQLAlchemy**: Database ORM and connection management
- **PostgreSQL**: Primary data warehouse
- **Apache Airflow**: Workflow orchestration

### Cloud & Infrastructure
- **AWS S3**: Object storage for data lakes and backups
- **AWS EC2**: Compute infrastructure for deployment
- **Docker**: Containerization for consistent environments
- **Docker Compose**: Multi-container orchestration

### Development & Testing
- **pytest**: Unit testing framework
- **black**: Code formatting
- **flake8**: Code linting
- **python-dotenv**: Environment configuration

## ğŸ“ Project Structure

```
data-engineering-pipeline/
â”œâ”€â”€ src/                          # Source code modules
â”‚   â”œâ”€â”€ extract/                  # Data extraction components
â”‚   â”‚   â”œâ”€â”€ csv_extractor.py      # CSV file extraction
â”‚   â”‚   â””â”€â”€ api_extractor.py      # API data extraction
â”‚   â”œâ”€â”€ transform/                # Data transformation components
â”‚   â”‚   â””â”€â”€ data_transformer.py   # Data cleaning and feature engineering
â”‚   â”œâ”€â”€ load/                     # Data loading components
â”‚   â”‚   â””â”€â”€ database_loader.py    # Database loading utilities
â”‚   â”œâ”€â”€ orchestration/            # Pipeline orchestration
â”‚   â”‚   â””â”€â”€ etl_pipeline.py       # Main ETL pipeline orchestrator
â”‚   â””â”€â”€ aws/                      # AWS integration
â”‚       â””â”€â”€ s3_integration.py      # S3 storage operations
â”œâ”€â”€ dags/                         # Airflow DAGs
â”‚   â””â”€â”€ data_pipeline_dag.py      # Main pipeline DAG
â”œâ”€â”€ config/                       # Configuration files
â”‚   â””â”€â”€ config.py                 # Application configuration
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ init_database.sql         # Database initialization
â”‚   â””â”€â”€ deploy_ec2.sh             # EC2 deployment script
â”œâ”€â”€ data/                         # Data directories
â”‚   â”œâ”€â”€ raw/                      # Raw input data
â”‚   â””â”€â”€ processed/                # Processed data
â”œâ”€â”€ tests/                        # Test files
â”œâ”€â”€ logs/                         # Log files
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .env.example                  # Environment variables template
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- PostgreSQL 12 or higher
- Docker and Docker Compose (for local development)
- AWS CLI and credentials (for cloud deployment)

### Local Setup

1. **Clone and Setup**
   ```bash
   git clone <repository-url>
   cd "data engg project"
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -r requirements.txt
   ```

2. **Configure Environment**
   ```bash
   cp .env.example .env
   # Edit .env with your configuration
   ```

3. **Setup Database**
   ```bash
   # Start PostgreSQL (using Docker)
   docker run --name postgres-pipeline -e POSTGRES_PASSWORD=password -e POSTGRES_DB=data_pipeline -p 5432:5432 -d postgres:13
   
   # Initialize database schema
   psql -h localhost -U postgres -d data_pipeline -f scripts/init_database.sql
   ```

4. **Run Pipeline**
   ```bash
   python src/orchestration/etl_pipeline.py
   ```

### Docker Setup

1. **Using Docker Compose**
   ```bash
   docker-compose up -d
   ```

2. **Access Services**
   - Airflow Web UI: http://localhost:8080
   - PostgreSQL: localhost:5432

### AWS Deployment

1. **Deploy to EC2**
   ```bash
   # Make sure AWS CLI is configured
   aws configure
   
   # Run deployment script
   ./scripts/deploy_ec2.sh
   ```

2. **Access Deployed Services**
   - Airflow Web UI: http://<EC2-PUBLIC-IP>:8080
   - PostgreSQL: <EC2-PUBLIC-IP>:5432

## ğŸ“Š Usage Examples

### CSV Data Pipeline

```python
from src.orchestration.etl_pipeline import ETLPipeline

# Initialize pipeline
pipeline = ETLPipeline("csv_pipeline")

# Configure CSV job
csv_config = {
    'users_job': {
        'type': 'csv',
        'table_name': 'raw_data.users',
        'file_paths': ['data/raw/users.csv'],
        'transformations': {
            'type_mapping': {
                'registration_date': 'datetime',
                'last_active': 'datetime'
            },
            'filters': {
                'age': {'min': 18, 'max': 100}
            }
        },
        'load_strategy': 'replace'
    }
}

# Run pipeline
results = pipeline.run_full_pipeline(csv_config)
print(f"Pipeline results: {results}")
```

### API Data Pipeline

```python
# Configure API job
api_config = {
    'products_job': {
        'type': 'api',
        'table_name': 'raw_data.products',
        'endpoint': 'products',
        'params': {'limit': 100},
        'transformations': {
            'type_mapping': {
                'created_date': 'datetime'
            }
        },
        'load_strategy': 'append'
    }
}

# Run pipeline
results = pipeline.run_full_pipeline(api_config)
```

### AWS S3 Integration

```python
from src.aws.s3_integration import S3Integration

# Initialize S3
s3 = S3Integration(
    aws_access_key_id='your_key',
    aws_secret_access_key='your_secret',
    bucket_name='data-pipeline-bucket'
)

# Upload data to S3
s3.upload_dataframe_to_s3(dataframe, 'raw/users.csv')

# Download data from S3
downloaded_df = s3.read_s3_to_dataframe('raw/users.csv')
```

## ğŸ”§ Configuration

### Environment Variables

Copy `.env.example` to `.env` and configure:

```bash
# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=data_pipeline
DB_USER=postgres
DB_PASSWORD=password

# AWS Configuration
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_REGION=us-east-1
S3_BUCKET_NAME=data-pipeline-bucket

# API Configuration
API_BASE_URL=https://api.example.com
API_KEY=your_api_key
```

### Database Schema

The pipeline uses a three-schema approach:

- **raw_data**: Raw extracted data
- **processed_data**: Transformed and aggregated data
- **analytics**: Business intelligence and metrics

## ğŸ“ˆ Features

### Data Extraction
- **CSV Files**: Multiple file support, automatic type inference
- **REST APIs**: Pagination support, error handling, retry logic
- **Error Recovery**: Comprehensive logging and retry mechanisms

### Data Transformation
- **Data Cleaning**: Duplicate removal, missing value handling
- **Type Conversion**: Automatic and manual type mapping
- **Feature Engineering**: Derived features, date extraction, categorical encoding
- **Data Validation**: Custom validation rules and quality checks

### Data Loading
- **Batch Processing**: Configurable chunk sizes for large datasets
- **Upsert Operations**: Conflict resolution and data updates
- **Performance**: Optimized loading with multi-insert operations

### Orchestration
- **Airflow Integration**: Professional workflow management
- **Scheduling**: Cron-based scheduling and dependency management
- **Monitoring**: Real-time pipeline status and alerting
- **Error Handling**: Automatic retries and failure notifications

### Cloud Integration
- **S3 Storage**: Data lake implementation with automatic archiving
- **EC2 Deployment**: One-click deployment to AWS
- **Scalability**: Horizontal scaling capabilities

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src

# Run specific test file
pytest tests/test_etl_pipeline.py
```

## ğŸ“Š Monitoring & Logging

### Pipeline Monitoring
- **Airflow UI**: Real-time pipeline status
- **Database Logs**: ETL job execution logs
- **System Logs**: Application and error logs

### Data Quality
- **Validation Rules**: Custom data quality checks
- **Metrics Tracking**: Automated quality metrics
- **Alerting**: Quality threshold violations

## ğŸ”’ Security

### Best Practices
- **Environment Variables**: Sensitive data in environment files
- **IAM Roles**: Principle of least privilege for AWS
- **Database Security**: Encrypted connections and access controls
- **API Security**: Key-based authentication

## ğŸš€ Production Deployment

### AWS Architecture
```
Internet Gateway
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EC2       â”‚    â”‚   RDS       â”‚    â”‚   S3        â”‚
â”‚             â”‚    â”‚             â”‚    â”‚             â”‚
â”‚ â€¢ Airflow   â”‚    â”‚ â€¢ PostgreSQLâ”‚    â”‚ â€¢ Data Lake â”‚
â”‚ â€¢ Pipeline  â”‚    â”‚ â€¢ Analytics â”‚    â”‚ â€¢ Backups   â”‚
â”‚ â€¢ Docker    â”‚    â”‚ â€¢ Raw Data  â”‚    â”‚ â€¢ Archive   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deployment Steps
1. **Infrastructure Setup**: VPC, security groups, IAM roles
2. **Application Deployment**: Docker containers on EC2
3. **Database Setup**: RDS PostgreSQL with automated backups
4. **Monitoring Setup**: CloudWatch, logging, and alerting
5. **Data Pipeline**: Scheduled ETL jobs with Airflow

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

For issues and questions:
1. Check the [Issues](../../issues) page
2. Create a new issue with detailed information
3. Include logs and configuration details

## ğŸ¯ Roadmap

- [ ] Real-time streaming with Apache Kafka
- [ ] Machine learning pipeline integration
- [ ] Advanced data visualization dashboard
- [ ] Multi-cloud deployment support
- [ ] GraphQL API for data access
- [ ] Advanced security features

---

**Built with â¤ï¸ for the data engineering community**
